{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SCAAI_Drive_Ankita Mandal_PS4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDAc7u5GDHDj"
      },
      "source": [
        "## **NLP based model to perform sentiment analysis on the corona virus tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSyZM_YgDTUe"
      },
      "source": [
        "#**Step 1: Importing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p29LuuP4DQ1J",
        "outputId": "d92b8151-a3b4-40a4-d51b-ab4fc71422e5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Text data\n",
        "import re\n",
        "import unicodedata\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "\n",
        "# Model\n",
        "from tensorflow.keras import layers, Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnUu3pKKEao5"
      },
      "source": [
        "#**Step 2: Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgpWyf4OJ6yW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "999cef32-9298-4340-feec-136b728ee57e"
      },
      "source": [
        "df_train = pd.read_csv(\"/content/drive/MyDrive/SCAAI_Drive_Ankita Mandal/SCAAI_Drive_Ankita Mandal_PS4 /Corona_NLP_train.csv\", encoding=\"latin_1\")    # encoding latin 1 will map all possible byte values to first 256 unicode points\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/SCAAI_Drive_Ankita Mandal/SCAAI_Drive_Ankita Mandal_PS4 /Corona_NLP_test.csv\", encoding=\"latin_1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-ea2c96047136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/SCAAI_Drive_Ankita Mandal/SCAAI_Drive_Ankita Mandal_PS4 /Corona_NLP_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin_1\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# encoding latin 1 will map all possible byte values to first 256 unicode points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/SCAAI_Drive_Ankita Mandal/SCAAI_Drive_Ankita Mandal_PS4 /Corona_NLP_test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1991\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/SCAAI_Drive_Ankita Mandal/SCAAI_Drive_Ankita Mandal_PS4 /Corona_NLP_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcGuvTC-VDpi"
      },
      "source": [
        "Copy of training data to preserve the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs5PjBEVClF"
      },
      "source": [
        "df_train_original = df_train.copy() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxrwxqRpJbSx"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA4n3L8RMzTy"
      },
      "source": [
        "# No. of tweets = 41157 in the training dataset\n",
        "df_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvMXHPmN8L-T"
      },
      "source": [
        "Copying test data to preserve the original data set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDgEuXWdUxRh"
      },
      "source": [
        "df_test_original=df_test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls50i1bH7AeS"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C7FzCnf671E"
      },
      "source": [
        "# No.of tweets in test dataset = 3798\n",
        "df_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY-PCS53AxIC"
      },
      "source": [
        "**Data Overview**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLmSg1uk7ECh"
      },
      "source": [
        "parameters = {'axes.labelsize': 20,\n",
        "              'axes.titlesize': 30}\n",
        "\n",
        "plt.rcParams.update(parameters)\n",
        "\n",
        "# A figure with 1 subplot\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(8, 4)\n",
        "\n",
        "#Group by sentiment\n",
        "df_train[\"Sentiment\"].reset_index().groupby(\"Sentiment\").count().rename(columns={\"index\": \"Count\"}).sort_values(by= \n",
        "       \"Count\").plot(kind=\"barh\", legend=False, \n",
        "        ax=ax).grid(axis='x')\n",
        "ax.tick_params(axis='x', labelsize=12)\n",
        "ax.tick_params(axis='y', labelsize=12)\n",
        "ax.set_ylabel(\"\")\n",
        "ax.set_title(\"Tweet sentiment count\", color =\"#292421\", fontsize= 16)\n",
        "fig.tight_layout(pad=2.0)\n",
        "plt.rcParams.update(parameters)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYvharzgEe9F"
      },
      "source": [
        "**Converting the above 5 sentiment categories into 3 categories** \n",
        "\n",
        "i.e here \"Extremely Positive\" and \"Extremely Negative\" will be converted to \"Positive\" and \"Negative\" respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQHJfVdlHSiF"
      },
      "source": [
        "def set_3_classes(x):\n",
        "  if x==\"Extremely Negative\":\n",
        "    return \"Negative\"\n",
        "  elif x==\"Extremely Positive\":\n",
        "    return \"Positive\"\n",
        "  else:\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iggu0BGHXzi"
      },
      "source": [
        "df_train[\"Sentiment\"] = df_train[\"Sentiment\"].apply(set_3_classes)\n",
        "df_test[\"Sentiment\"] = df_test[\"Sentiment\"].apply(set_3_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdNILJLOHlul"
      },
      "source": [
        "Plotting the changes in sentiment classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q24mWYjVHbF3"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.suptitle(\"Count\", fontsize=16)\n",
        "df_train[\"Sentiment\"].reset_index().groupby(\"Sentiment\").count().sort_values(by= \n",
        "       \"index\").plot(kind=\"barh\", legend=False, \n",
        "        ax=ax).grid(axis='x')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "143MIWlYPvk1"
      },
      "source": [
        "labels=['Negative', 'Neutral', 'Positive']\n",
        "sizes = [\n",
        "         \n",
        "         df_train[df_train['Sentiment'] == 'Negative'].shape[0], \n",
        "         df_train[df_train['Sentiment'] == 'Neutral'].shape[0],\n",
        "         df_train[df_train['Sentiment'] == 'Positive'].shape[0]\n",
        "        ]\n",
        "plt.pie(sizes,labels=labels, data=df_train, autopct='%1.2f%%', shadow=True, startangle=90)\n",
        "plt.title(\"Sentiments percentages in train data\")\n",
        "plt.axis(\"equal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxtBXQD0MgzR"
      },
      "source": [
        "#**Step 3: Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaLgdpbpNTTO"
      },
      "source": [
        " **Cleaning Training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWlznYKlNSFv"
      },
      "source": [
        "df_train[\"CleanTweet\"] = df_train[\"OriginalTweet\"]\n",
        "df_train.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9TTetNqRAPd"
      },
      "source": [
        "*We only need 4 columns which are Location, TweetAt, Original T\n",
        "weet and Sentiment column*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYZ0JUZx23cb"
      },
      "source": [
        "df_train = df_train.iloc[:,2:]\n",
        "df_test = df_test.iloc[:,2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oSvXXkR3CaF"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Zc8kNwU-4D"
      },
      "source": [
        "**Removing end-of-line, tabulation and carriage return. Turning into lower case**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6XvXjpEHiKT"
      },
      "source": [
        "def clean_eol_tabs(df, label):\n",
        "    \"\"\" text lowercase\n",
        "        removes \\n\n",
        "        removes \\t\n",
        "        removes \\r \"\"\"\n",
        "    df[label] = df[label].str.lower()\n",
        "    df[label] = df[label].apply(lambda x: x.replace(\"\\n\", \" \"))\n",
        "    df[label] = df[label].apply(lambda x: x.replace(\"\\r\", \" \"))\n",
        "    df[label] = df[label].apply(lambda x: x.replace(\"\\t\", \" \"))\n",
        "    return df\n",
        "\n",
        "df_train = clean_eol_tabs(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBExg64wVhWk"
      },
      "source": [
        "**Removing e-mails**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-mBhPd1Rf6X"
      },
      "source": [
        "def remove_emails(df, label):\n",
        "    \"\"\" This function removes email adresses inputs: - text \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: re.sub(r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\", \" \", x))\n",
        "    return df\n",
        "\n",
        "df_train = remove_emails(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YblfYR1VzIJ"
      },
      "source": [
        "**Removing mentions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alhcW1oMV4MC"
      },
      "source": [
        "def remove_mentions(df, label):\n",
        "    \"\"\" This function removes mentions (Twitter - starting with @) from texts inputs:  - text \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: re.sub(r\"@([a-zA-Z0-9_.-]{1,100})\", \" \", x))\n",
        "    return df\n",
        "\n",
        "df_train = remove_mentions(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bplb26FnWM7L"
      },
      "source": [
        "**Removing hyperlinks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtTwCg3PWSMc"
      },
      "source": [
        "def remove_hyperlinks(df, label):\n",
        "    \"\"\" This function removes hyperlinks from texts\n",
        "        inputs:\n",
        "         - text \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: re.sub(r\"http\\S+\", \" \", x))\n",
        "    return df\n",
        "\n",
        "df_train = remove_hyperlinks(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdJlemNFWXA4"
      },
      "source": [
        "**Removing hashtags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8bfUpFmWWlv"
      },
      "source": [
        "def remove_hashtags(df, label):\n",
        "    \"\"\" This function removes hashtags\n",
        "        inputs:\n",
        "         - text \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: re.sub(r\"#\\w+\", \" \", x))\n",
        "    return df\n",
        "\n",
        "df_train = remove_hashtags(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LVFe-PX7fK"
      },
      "source": [
        "**Removing html tags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuTppgu5X_kA"
      },
      "source": [
        "def remove_html_tags(df, label):\n",
        "    \"\"\" This function removes html tags from texts\n",
        "        inputs:\n",
        "         - text \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: re.sub(r\"<.*?>\", \" \", x))\n",
        "    return df\n",
        "\n",
        "df_train = remove_html_tags(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLl9c6VhYESB"
      },
      "source": [
        "**Removing numbers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfJdhBoKYI1T"
      },
      "source": [
        "def remove_numbers(df, label):\n",
        "    \"\"\" This function removes numbers from a text\n",
        "        inputs:\n",
        "         - text \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n",
        "    return df\n",
        "#\n",
        "df_train = remove_numbers(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQbL1BhOYOZo"
      },
      "source": [
        "**Encode unknown characters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jALSLT2YTdC"
      },
      "source": [
        "def encode_unknown(df, label):\n",
        "    \"\"\" This function encodes special caracters \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: unicodedata.normalize(\"NFD\", x).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
        "    return df\n",
        "\n",
        "df_train = encode_unknown(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kExxBZ3YXIp"
      },
      "source": [
        "**Removing punctuations and special characters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P51ou3L_YWtH"
      },
      "source": [
        "def clean_punctuation_no_accent(df, label):\n",
        "    \"\"\" This function removes punctuation and accented characters from texts in a dataframe \n",
        "        To be appplied to languages that have no accents, ex: english \n",
        "    \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
        "    return df\n",
        "\n",
        "df_train = clean_punctuation_no_accent(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Xn7SorYfeY"
      },
      "source": [
        "**Removing one and two letters words, removing unnecessary spaces, droping empty lines**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vwCvsiAY36F"
      },
      "source": [
        "def more_cleaning(df, label):\n",
        "    \"\"\" This function\n",
        "     1) removes remaining one-letter words and two letters words\n",
        "     2) replaces multiple spaces by one single space\n",
        "     3) drop empty lines \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', \" \", x))\n",
        "    df[label] = df[label].apply(lambda x: re.sub(r\"[ \\t]{2,}\", \" \", x))\n",
        "    df[label] = df[label].apply(lambda x: x if len(x) != 1 else '')\n",
        "    df[label] = df[label].apply(lambda x: np.nan if x == '' else x)\n",
        "    df = df.dropna(subset=[label], axis=0).reset_index(drop=True).copy()\n",
        "    return df\n",
        "\n",
        "df_train = more_cleaning(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGXMYYC5goxy"
      },
      "source": [
        " ***Lexical Analysis***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGd5xMm8aDNV"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uAdinnTaF6m"
      },
      "source": [
        "tokenized_tweet = df_train['CleanTweet'].apply(lambda x: x.split())\n",
        "tokenized_tweet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrJqwD3_mh0u"
      },
      "source": [
        "**Removing Stop Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBUn5K6smm5c"
      },
      "source": [
        "def remove_stop_words(text, stopwords=set(stopwords.words('english'))):\n",
        "    \"\"\" This function removes stop words from a text\n",
        "        inputs:\n",
        "         - stopword list\n",
        "         - text \"\"\"\n",
        "\n",
        "    # prepare new text\n",
        "    text_splitted = text.split(\" \")\n",
        "    text_new = list()\n",
        "    \n",
        "    # stop words updated\n",
        "    #stopwords = stopwords.union({\"amp\", \"grocery store\", \"covid\", \"supermarket\", \"people\", \"grocery\", \"store\", \"price\", \"time\", \"consumer\"})\n",
        "    \n",
        "    # loop\n",
        "    for word in text_splitted:\n",
        "        if word not in stopwords:\n",
        "            text_new.append(word)\n",
        "    return \" \".join(text_new)\n",
        "\n",
        "def clean_stopwords(df, label):\n",
        "    \"\"\" This function removes stopwords \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: remove_stop_words(x))\n",
        "    return df\n",
        "#\n",
        "df_train = clean_stopwords(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANrioL-Tg6S-"
      },
      "source": [
        "***Syntactic Analysis***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz3cJ0HBaX4z"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8MhCgTYy3fX"
      },
      "source": [
        "from nltk import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [ps.stem(i) for i in x])\n",
        "\n",
        "tokenized_tweet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQa_quaDN62f"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgNeuS9ANXSt"
      },
      "source": [
        "def lemmatize_one_text(text):\n",
        "    \"\"\" This function lemmatizes words in text (it changes word to most close root word)\n",
        "        inputs:\n",
        "         - lemmatizer\n",
        "         - text \"\"\"\n",
        "\n",
        "    # initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    # tags\n",
        "    lem_tags = ['a', 'r', 'n', 'v']\n",
        "\n",
        "    # prepare new text\n",
        "    text_splitted = text.split(\" \")\n",
        "    text_new = list()\n",
        "\n",
        "    # change bool\n",
        "    changed = ''\n",
        "    \n",
        "    # loop\n",
        "    for word in text_splitted:\n",
        "        text_new.append(lemmatizer.lemmatize(word))\n",
        "        #changed = ''\n",
        "        #for tag in lem_tags:\n",
        "        #    if lemmatizer.lemmatize(word, tag) != word:\n",
        "        #        changed = tag\n",
        "        #if changed == '':\n",
        "        #    text_new.append(word)\n",
        "        #else:\n",
        "        #    text_new.append(lemmatizer.lemmatize(word, changed))\n",
        "\n",
        "    return \" \".join(text_new)\n",
        "\n",
        "def lemmatize(df, label):\n",
        "    \"\"\" This function lemmatizes texts \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: lemmatize_one_text(x))\n",
        "    return df\n",
        "#\n",
        "df_train = lemmatize(df_train, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jND2JwC-2cev"
      },
      "source": [
        "df_train.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL2rLMTZ46bq"
      },
      "source": [
        "***Similarly applying cleaning techniques on Test data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4vYxYp05GIq"
      },
      "source": [
        "df_test[\"CleanTweet\"] = df_test[\"OriginalTweet\"]\n",
        "df_test = clean_eol_tabs(df_test, \"CleanTweet\")\n",
        "df_test = remove_emails(df_test, \"CleanTweet\")\n",
        "df_test = remove_mentions(df_test, \"CleanTweet\")\n",
        "df_test = remove_hyperlinks(df_test, \"CleanTweet\")\n",
        "df_test = remove_hashtags(df_test, \"CleanTweet\")\n",
        "df_test = remove_html_tags(df_test, \"CleanTweet\")\n",
        "df_test = remove_numbers(df_test, \"CleanTweet\")\n",
        "df_test = encode_unknown(df_test, \"CleanTweet\")\n",
        "df_test = clean_punctuation_no_accent(df_test, \"CleanTweet\")\n",
        "df_test = more_cleaning(df_test, \"CleanTweet\")\n",
        "df_test = lemmatize(df_test, \"CleanTweet\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4SJfSEA54Jj"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMu79meh56UB"
      },
      "source": [
        "tokenized_tweet = df_test['CleanTweet'].apply(lambda x: x.split())\n",
        "tokenized_tweet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y0nXGqa6OFm"
      },
      "source": [
        "**Removing Stop Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp_wy8XK6QQn"
      },
      "source": [
        "def remove_stop_words(text, stopwords=set(stopwords.words('english'))):\n",
        "    \"\"\" This function removes stop words from a text\n",
        "        inputs:\n",
        "         - stopword list\n",
        "         - text \"\"\"\n",
        "\n",
        "    # prepare new text\n",
        "    text_splitted = text.split(\" \")\n",
        "    text_new = list()\n",
        "    \n",
        "    # stop words updated\n",
        "    #stopwords = stopwords.union({\"amp\", \"grocery store\", \"covid\", \"supermarket\", \"people\", \"grocery\", \"store\", \"price\", \"time\", \"consumer\"})\n",
        "    \n",
        "    # loop\n",
        "    for word in text_splitted:\n",
        "        if word not in stopwords:\n",
        "            text_new.append(word)\n",
        "    return \" \".join(text_new)\n",
        "\n",
        "def clean_stopwords(df, label):\n",
        "    \"\"\" This function removes stopwords \"\"\"\n",
        "    df[label] = df[label].apply(lambda x: remove_stop_words(x))\n",
        "    return df\n",
        "#\n",
        "df_test = clean_stopwords(df_test, \"CleanTweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVp4MAuK6yTY"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyKF1XuX6zoJ"
      },
      "source": [
        "from nltk import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [ps.stem(i) for i in x])\n",
        "\n",
        "tokenized_tweet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu0vbI-vNNVC"
      },
      "source": [
        "**Lemmaztization**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp8lYI0H7BxQ"
      },
      "source": [
        "df_test.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNKB397x8Ari"
      },
      "source": [
        "**Before/after cleaning on several tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9qfKyVX76pi"
      },
      "source": [
        "import random\n",
        "tweet_num = random.randint(0, df_train.shape[1])\n",
        "print(\"############################# Original Tweet #############################\")\n",
        "print(df_train.iloc[tweet_num].at[\"OriginalTweet\"])\n",
        "print(\"\\n\")\n",
        "print(\"############################# Clean Tweet ################################\")\n",
        "print(df_train.iloc[tweet_num].at[\"CleanTweet\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nptNML4D9Si0"
      },
      "source": [
        "#**Step 4: Data Visualization with the help of word cloud**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNLGH6zh9pjw"
      },
      "source": [
        "**Word cloud in each sentiment category**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KtqsmPV8EkP"
      },
      "source": [
        "all_words_positive = \" \".join([text for text in df_train[df_train[\"Sentiment\"]==\"Positive\"][\"CleanTweet\"]])\n",
        "all_words_neutral = \" \".join([text for text in df_train[df_train[\"Sentiment\"]==\"Neutral\"][\"CleanTweet\"]])\n",
        "all_words_negative = \" \".join([text for text in df_train[df_train[\"Sentiment\"]==\"Negative\"][\"CleanTweet\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFAGVoAq9vy1"
      },
      "source": [
        "wordcloud_positive = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"Greens\").generate(all_words_positive)\n",
        "wordcloud_neutral = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"Blues\").generate(all_words_neutral)\n",
        "wordcloud_negative = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"Reds\").generate(all_words_negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55e_7c_a95Ys"
      },
      "source": [
        "parameters = {'axes.labelsize': 12,\n",
        "              'axes.titlesize': 10}\n",
        "\n",
        "# A figure with 3 subplots\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
        "fig.set_size_inches(18.5, 7)\n",
        "ax1.imshow(wordcloud_positive, interpolation='bilinear')\n",
        "ax1.axis(\"off\")\n",
        "ax1.set_title(\"WordCloud of positive tweets\", fontsize=12)\n",
        "ax2.imshow(wordcloud_neutral, interpolation='bilinear')\n",
        "ax2.axis(\"off\")\n",
        "ax2.set_title(\"WordCloud of neutral tweets\", fontsize=12)\n",
        "ax3.imshow(wordcloud_negative, interpolation='bilinear')\n",
        "ax3.axis(\"off\")\n",
        "ax3.set_title(\"WordCloud of negative tweets\", fontsize=12)\n",
        "plt.rcParams.update(parameters)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2valy67ROh03"
      },
      "source": [
        "#**Step 5: Model Creation and training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW0rrpYwquIz"
      },
      "source": [
        "**Sentiment Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZIH2XZqo6-3"
      },
      "source": [
        "df_train_encoded = df_train.copy()\n",
        "df_test_encoded = df_test.copy()\n",
        "\n",
        "print(\"train set shape: \" + str(df_train_encoded.shape))\n",
        "print(\"test set shape: \" + str(df_test_encoded.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UzZJPIE-LvA"
      },
      "source": [
        "map_sentiment = {\"Neutral\":0, \"Positive\":1,\"Negative\":2}\n",
        "df_train_encoded['Sentiment'] = df_train_encoded['Sentiment'].map(map_sentiment)\n",
        "df_test_encoded['Sentiment']  = df_test_encoded['Sentiment'].map(map_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS_r_bPosG2j"
      },
      "source": [
        "# Target Preparation\n",
        "y_train = df_train['Sentiment'].copy()\n",
        "y_test = df_test['Sentiment'].copy()\n",
        "\n",
        "y_train_encoded = to_categorical(df_train_encoded['Sentiment'], 3)\n",
        "y_test_encoded = to_categorical(df_test_encoded['Sentiment'], 3)\n",
        "\n",
        "y_train_mapped = df_train_encoded['Sentiment'].copy()\n",
        "y_test_mapped = df_test_encoded['Sentiment'].copy()\n",
        "\n",
        "X_train = df_train_encoded[['CleanTweet']].copy()\n",
        "X_test = df_test_encoded[['CleanTweet']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1FtjZF9sUse"
      },
      "source": [
        "**Tokens, sequence and padding**\n",
        "\n",
        "->key = word\n",
        "\n",
        "->value = unique number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrfmJGWwukv4"
      },
      "source": [
        " *a. Tokens*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kBKUn_FsjaX"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train[\"CleanTweet\"])\n",
        "vocab_length = len(tokenizer.word_index) + 1\n",
        "vocab_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy6XZ0FJs_TN"
      },
      "source": [
        "# texts_to_sequences function first transforms a text into list of words\n",
        "X_train = tokenizer.texts_to_sequences(X_train[\"CleanTweet\"])\n",
        "X_test = tokenizer.texts_to_sequences(X_test[\"CleanTweet\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhtDhybhtrui"
      },
      "source": [
        "# texts_to_sequences function will be a list of list of numbers of varying length, since different tweets have different lengths\n",
        "print(\"First tweet encoded:\")\n",
        "print(X_train[0])\n",
        "print(\"\\nSecond tweet encoded:\")\n",
        "print(X_train[1])\n",
        "print(\"\\nThird tweet encoded:\")\n",
        "print(X_train[2])\n",
        "print(\"\\nFourth tweet encoded:\")\n",
        "print(X_train[3])\n",
        "print(\"\\nFifth tweet encoded:\")\n",
        "print(X_train[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrrurNXZuU1d"
      },
      "source": [
        "**Maximum no.of words in one tweet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwjA0IOxuMr1"
      },
      "source": [
        "max_word_count = 0\n",
        "word_count = []\n",
        "#\n",
        "for encoded_tweet in X_train:\n",
        "    word_count.append(len(encoded_tweet))\n",
        "    if len(encoded_tweet) > max_word_count:\n",
        "        max_word_count = len(encoded_tweet)\n",
        "print(\"Maximum number of word in one tweet: \" + str(max_word_count) + \" words\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYuI-k_husX3"
      },
      "source": [
        "*b. Padding*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxz4SO49uxXS"
      },
      "source": [
        "# pad the sequences with a maximum length of 37 since the max word count is 37\n",
        "X_train = pad_sequences(X_train, maxlen=max_word_count, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_word_count, padding='post')\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giVuvtwKvGi2"
      },
      "source": [
        "# All encoded tweets are of the same length\n",
        "print(\"First tweet encoded:\", \"Size = \", len(X_train[0]))\n",
        "print(X_train[0])\n",
        "print(\"\\nSecond tweet encoded:\", \"Size = \", len(X_train[1]))\n",
        "print(X_train[1])\n",
        "print(\"\\nThird tweet encoded:\", \"Size = \", len(X_train[2]))\n",
        "print(X_train[2])\n",
        "print(\"\\nFourth tweet encoded:\", \"Size = \", len(X_train[3]))\n",
        "print(X_train[3])\n",
        "print(\"\\nFifth tweet encoded:\", \"Size = \", len(X_train[4]))\n",
        "print(X_train[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs3lr-KBo7yz"
      },
      "source": [
        "**LSTM**\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2cR57LOvpbT"
      },
      "source": [
        "model_LSTM = Sequential()\n",
        "model_LSTM.add(layers.Embedding(vocab_length, output_dim=32, input_length=max_word_count, mask_zero=True))\n",
        "model_LSTM.add(layers.LSTM(100))\n",
        "model_LSTM.add(layers.Dense(64, activation=\"relu\"))\n",
        "model_LSTM.add(layers.Dense(32, activation=\"relu\"))\n",
        "model_LSTM.add(layers.Dense(16, activation=\"relu\"))\n",
        "model_LSTM.add(layers.Dense(3, activation='softmax'))\n",
        "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_LSTM.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5vnXxEHxjma"
      },
      "source": [
        "# restore_best_weights to True so that the weights of best score on monitored metric\n",
        "#val_accuracy i.e accuracy on test set - are restored when training stops\n",
        "es = EarlyStopping(patience=10, monitor='val_accuracy', restore_best_weights=True)\n",
        "history = model_LSTM.fit(X_train,\n",
        "                         y_train_encoded,\n",
        "                         validation_data=(X_test, y_test_encoded),\n",
        "                         epochs=5,\n",
        "                         batch_size=16,\n",
        "                         verbose=1,\n",
        "                         callbacks=[es]\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKDeQD1U0ShO"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('LOSS',fontdict={'size':'22'})\n",
        "plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPjI7WujABSQ"
      },
      "source": [
        "#**Step 6: Accuracy on test data set**\n",
        "\n",
        "*For LSTM Algo*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnPRPtGHAuOb"
      },
      "source": [
        "# prediction on test data set\n",
        "predicted = model_LSTM.predict(X_test)\n",
        "y_pred = predicted.argmax(axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkVv_3odA6xy"
      },
      "source": [
        "**Accuracy and Area Under (ROC) Curve - AUC - scores**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aHJWz4nA2Dw"
      },
      "source": [
        "acc_score = accuracy_score(y_test_mapped, y_pred)\n",
        "auc_score = roc_auc_score(y_test_mapped, predicted, multi_class=\"ovr\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8QihC7o-_sp"
      },
      "source": [
        "report = classification_report(y_test_mapped, y_pred, target_names=list(y_test.unique()), output_dict=True)\n",
        "accuracy_col = ([\"\"]*3) + [round(acc_score, 2)]\n",
        "roc_auc_col = ([\"\"]*3) + [round(auc_score, 2)]\n",
        "accuracy_col = pd.Series(accuracy_col, index=list(report[\"Neutral\"].keys()))\n",
        "roc_auc_col = pd.Series(roc_auc_col, index=list(report[\"Neutral\"].keys()))\n",
        "df_report = pd.DataFrame(report)[[\"Neutral\", \"Positive\", \"Negative\", \"macro avg\", \"weighted avg\"]].apply(lambda x: round(x, 2))\n",
        "df_report[\"accuracy\"] = accuracy_col\n",
        "df_report[\"roc_auc\"] = roc_auc_col\n",
        "df_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgG3PiWWC8s0"
      },
      "source": [
        "**From the report generated above we can infer that 85% accuracy is obtained on test data set.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YaYMwwOxp97"
      },
      "source": [
        "#**Applying Machine Learning Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml-jccoybTR3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train,valid = train_test_split(df_train,test_size = 0.2,random_state=0,stratify = df_train.Sentiment.values) #stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n",
        "print(\"train shape : \", train.shape)\n",
        "print(\"valid shape : \", valid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0_EiD3lc-Pn"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "stop = list(stopwords.words('english'))\n",
        "vectorizer = CountVectorizer(decode_error = 'replace',stop_words = stop)\n",
        "\n",
        "X_train = vectorizer.fit_transform(train.CleanTweet.values)\n",
        "X_valid = vectorizer.transform(valid.CleanTweet.values)\n",
        "\n",
        "y_train = train.Sentiment.values\n",
        "y_valid = valid.Sentiment.values\n",
        "\n",
        "print(\"X_train.shape : \", X_train.shape)\n",
        "print(\"X_train.shape : \", X_valid.shape)\n",
        "print(\"y_train.shape : \", y_train.shape)\n",
        "print(\"y_valid.shape : \", y_valid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWqmVY8MQQgH"
      },
      "source": [
        "**a. Naive Bayes Classifier for MULTICLASS Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F71fVwCBdf5F"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "naiveByes_clf = MultinomialNB()\n",
        "\n",
        "naiveByes_clf.fit(X_train,y_train)\n",
        "\n",
        "NB_prediction = naiveByes_clf.predict(X_valid)\n",
        "NB_accuracy = accuracy_score(y_valid,NB_prediction)\n",
        "print(\"training accuracy Score    : \",naiveByes_clf.score(X_train,y_train))\n",
        "print(\"Validation accuracy Score : \",NB_accuracy )\n",
        "print(classification_report(NB_prediction,y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEc2SHV1eOR3"
      },
      "source": [
        "**b. Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cod5tJidtBh"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_clf = RandomForestClassifier()\n",
        "\n",
        "rf_clf.fit(X_train,y_train)\n",
        "\n",
        "rf_prediction = rf_clf.predict(X_valid)\n",
        "rf_accuracy = accuracy_score(y_valid,rf_prediction)\n",
        "print(\"Training accuracy Score    : \",rf_clf.score(X_train,y_train))\n",
        "print(\"Validation accuracy Score : \",rf_accuracy )\n",
        "print(classification_report(rf_prediction,y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6x4UX74e_XG"
      },
      "source": [
        "**c. Support vector machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se2ko5zce-z-"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC()\n",
        "\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "svc_prediction = svc.predict(X_valid)\n",
        "svc_accuracy = accuracy_score(y_valid,svc_prediction)\n",
        "print(\"Training accuracy Score    : \",svc.score(X_train,y_train))\n",
        "print(\"Validation accuracy Score : \",svc_accuracy )\n",
        "print(classification_report(svc_prediction,y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C_g432gf35G"
      },
      "source": [
        "#**ML Model Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBFlNEHSf5Iv"
      },
      "source": [
        "models = pd.DataFrame({\n",
        "    'Model': ['Support Vector Machines', \n",
        "              'Random Forest', 'Naive Bayes',],\n",
        "    'Test accuracy': [svc_accuracy, rf_accuracy, NB_accuracy,]})\n",
        "\n",
        "models.sort_values(by='Test accuracy', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}